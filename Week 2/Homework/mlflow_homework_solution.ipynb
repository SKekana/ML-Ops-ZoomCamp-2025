{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768534e1",
   "metadata": {},
   "source": [
    "# MLflow Homework Solutions\n",
    "\n",
    "This notebook contains solutions to the MLFlow homework from ML-Ops-ZoomCamp-2025 Week 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55753d24",
   "metadata": {},
   "source": [
    "## Q1. Install MLflow\n",
    "\n",
    "To get started with MLflow, we need to install the MLflow Python package and check its version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MLflow if not already installed\n",
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193268c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check MLflow version\n",
    "!mlflow --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9daeb54",
   "metadata": {},
   "source": [
    "**Answer to Q1**: The version of MLflow I have is 2.10.0 (this might differ based on when you install it)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe020ac6",
   "metadata": {},
   "source": [
    "## Q2. Download and Preprocess the Data\n",
    "\n",
    "We need to download the Yellow Taxi Trip Records dataset for January, February, and March 2023, and then preprocess the data using the provided script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472dea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for taxi data if it doesn't exist\n",
    "import os\n",
    "\n",
    "data_dir = \"taxi_data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Download the data for January, February and March 2023\n",
    "!wget -P {data_dir} https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\n",
    "!wget -P {data_dir} https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet\n",
    "!wget -P {data_dir} https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple version of preprocess_data.py if it doesn't exist\n",
    "\n",
    "preprocess_script = '''\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def dump_pickle(obj, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        return pickle.dump(obj, f)\n",
    "\n",
    "def read_dataframe(filename):\n",
    "    df = pd.read_parquet(filename)\n",
    "    \n",
    "    df[\"duration\"] = df[\"tpep_dropoff_datetime\"] - df[\"tpep_pickup_datetime\"]\n",
    "    df[\"duration\"] = df[\"duration\"].dt.total_seconds() / 60\n",
    "\n",
    "    df = df[((df[\"duration\"] >= 1) & (df[\"duration\"] <= 60))]\n",
    "\n",
    "    categorical = [\"PULocationID\", \"DOLocationID\"]\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess(df, dv=None):\n",
    "    df[\"PU_DO\"] = df[\"PULocationID\"] + \"_\" + df[\"DOLocationID\"]\n",
    "    categorical = [\"PU_DO\"]\n",
    "    numerical = [\"trip_distance\"]\n",
    "    dicts = df[categorical + numerical].to_dict(orient=\"records\")\n",
    "    \n",
    "    if dv is None:\n",
    "        dv = DictVectorizer()\n",
    "        X = dv.fit_transform(dicts)\n",
    "    else:\n",
    "        X = dv.transform(dicts)\n",
    "    \n",
    "    return X, dv\n",
    "\n",
    "def main(raw_data_path, dest_path):\n",
    "    # Load the data\n",
    "    jan_data = os.path.join(raw_data_path, \"yellow_tripdata_2023-01.parquet\")\n",
    "    feb_data = os.path.join(raw_data_path, \"yellow_tripdata_2023-02.parquet\")\n",
    "    mar_data = os.path.join(raw_data_path, \"yellow_tripdata_2023-03.parquet\")\n",
    "    \n",
    "    # Create the destination path if it doesn\\'t exist\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "    \n",
    "    # Read the data\n",
    "    df_train = read_dataframe(jan_data)\n",
    "    df_val = read_dataframe(feb_data)\n",
    "    df_test = read_dataframe(mar_data)\n",
    "    \n",
    "    # Extract the target\n",
    "    target = \"duration\"\n",
    "    y_train = df_train[target].values\n",
    "    y_val = df_val[target].values\n",
    "    y_test = df_test[target].values\n",
    "    \n",
    "    # Preprocess the data\n",
    "    X_train, dv = preprocess(df_train)\n",
    "    X_val, _ = preprocess(df_val, dv)\n",
    "    X_test, _ = preprocess(df_test, dv)\n",
    "    \n",
    "    # Save the datasets and DictVectorizer\n",
    "    dump_pickle(dv, os.path.join(dest_path, \"dv.pkl\"))\n",
    "    dump_pickle((X_train, y_train), os.path.join(dest_path, \"train.pkl\"))\n",
    "    dump_pickle((X_val, y_val), os.path.join(dest_path, \"val.pkl\"))\n",
    "    dump_pickle((X_test, y_test), os.path.join(dest_path, \"test.pkl\"))\n",
    "    \n",
    "    # Count the number of files saved\n",
    "    print(f\"Number of files saved: {len(os.listdir(dest_path))}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--raw_data_path\",\n",
    "        help=\"the location where the raw data is stored\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dest_path\",\n",
    "        help=\"the location where the resulting files will be saved\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.raw_data_path, args.dest_path)\n",
    "'''\n",
    "\n",
    "if not os.path.exists(\"preprocess_data.py\"):\n",
    "    with open(\"preprocess_data.py\", \"w\") as f:\n",
    "        f.write(preprocess_script)\n",
    "    print(\"Created preprocess_data.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb5269a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Run the preprocessing script\n",
    "!python preprocess_data.py --raw_data_path {data_dir} --dest_path {output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9025da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the files in the output directory\n",
    "output_files = os.listdir(output_dir)\n",
    "print(f\"Files in output directory: {output_files}\")\n",
    "print(f\"Number of files: {len(output_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c03e61",
   "metadata": {},
   "source": [
    "**Answer to Q2**: 4 files were saved to the output folder:\n",
    "- dv.pkl (the DictVectorizer)\n",
    "- train.pkl (January 2023 data)\n",
    "- val.pkl (February 2023 data)\n",
    "- test.pkl (March 2023 data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c740474",
   "metadata": {},
   "source": [
    "## Q3. Train a Model with Autolog\n",
    "\n",
    "We will train a RandomForestRegressor on the taxi dataset with MLflow autologging enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce4549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training script with autologging\n",
    "\n",
    "train_script = '''\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Enable autologging\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"nyc-taxi-experiment\")\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def main(data_path):\n",
    "    with mlflow.start_run():\n",
    "        # Load the data\n",
    "        X_train, y_train = load_pickle(os.path.join(data_path, \"train.pkl\"))\n",
    "        X_val, y_val = load_pickle(os.path.join(data_path, \"val.pkl\"))\n",
    "\n",
    "        # Train a model\n",
    "        rf = RandomForestRegressor(max_depth=10, random_state=0)\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = rf.predict(X_val)\n",
    "\n",
    "        # Calculate RMSE\n",
    "        rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        print(f\"RMSE: {rmse}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        default=\"./output\",\n",
    "        help=\"the location where the preprocessed data is stored\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.data_path)\n",
    "'''\n",
    "\n",
    "if not os.path.exists(\"train.py\"):\n",
    "    with open(\"train.py\", \"w\") as f:\n",
    "        f.write(train_script)\n",
    "    print(\"Created train.py with autologging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabebf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training script\n",
    "!python train.py --data_path {output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cacd213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the MLflow data to find the min_samples_split parameter\n",
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(\"mlflow.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query to get the latest run and its parameters\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT params \n",
    "    FROM params \n",
    "    WHERE key = 'min_samples_split' \n",
    "    ORDER BY run_uuid DESC \n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "result = cursor.fetchone()\n",
    "if result:\n",
    "    min_samples_split = result[0]\n",
    "    print(f\"min_samples_split parameter: {min_samples_split}\")\n",
    "else:\n",
    "    print(\"min_samples_split parameter not found in the database.\")\n",
    "\n",
    "# Alternative approach - if we have mlflow API access\n",
    "try:\n",
    "    import mlflow\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    runs = client.search_runs(\"nyc-taxi-experiment\")\n",
    "    if runs:\n",
    "        run = runs[0]\n",
    "        params = run.data.params\n",
    "        if \"min_samples_split\" in params:\n",
    "            print(f\"min_samples_split parameter (from API): {params['min_samples_split']}\")\n",
    "        else:\n",
    "            print(\"min_samples_split parameter not found in the run data.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing MLflow API: {e}\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277f1d6",
   "metadata": {},
   "source": [
    "**Answer to Q3**: The value of the `min_samples_split` parameter is 2, which is the default value for RandomForestRegressor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53ce53c",
   "metadata": {},
   "source": [
    "## Q4. Launch the Tracking Server Locally\n",
    "\n",
    "To manage the entire lifecycle of our ML model, we need to launch a tracking server with access to the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987359ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create artifacts directory if it doesn't exist\n",
    "artifacts_dir = \"artifacts\"\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "\n",
    "# Command to launch the tracking server\n",
    "tracking_server_command = f\"mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root {artifacts_dir} --host 0.0.0.0 --port 5000\"\n",
    "\n",
    "print(\"To launch the tracking server, run the following command in a separate terminal:\")\n",
    "print(tracking_server_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57ba08f",
   "metadata": {},
   "source": [
    "**Answer to Q4**: In addition to `backend-store-uri`, we need to pass `default-artifact-root` to properly configure the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf89931",
   "metadata": {},
   "source": [
    "## Q5. Tune Model Hyperparameters\n",
    "\n",
    "We'll tune the hyperparameters of the RandomForestRegressor using hyperopt and log the results to MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec9c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a hyperparameter tuning script\n",
    "\n",
    "hpo_script = '''\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"random-forest-hyperopt\")\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f_in:\n",
    "        return pickle.load(f_in)\n",
    "\n",
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        # Log hyperparameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        rf = RandomForestRegressor(**params)\n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_val)\n",
    "        rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "        \n",
    "        # Log the RMSE\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        \n",
    "        return {\"loss\": rmse, \"status\": STATUS_OK}\n",
    "\n",
    "def run(data_path, num_trials=20):\n",
    "    global X_train, y_train, X_val, y_val\n",
    "    \n",
    "    X_train, y_train = load_pickle(os.path.join(data_path, \"train.pkl\"))\n",
    "    X_val, y_val = load_pickle(os.path.join(data_path, \"val.pkl\"))\n",
    "\n",
    "    search_space = {\n",
    "        \"max_depth\": scope.int(hp.quniform(\"max_depth\", 1, 20, 1)),\n",
    "        \"n_estimators\": scope.int(hp.quniform(\"n_estimators\", 10, 50, 1)),\n",
    "        \"min_samples_split\": scope.int(hp.quniform(\"min_samples_split\", 2, 10, 1)),\n",
    "        \"min_samples_leaf\": scope.int(hp.quniform(\"min_samples_leaf\", 1, 4, 1)),\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    rstate = np.random.default_rng(42)  # for reproducible results\n",
    "    trials = Trials()\n",
    "    best = fmin(\n",
    "        fn=objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=num_trials,\n",
    "        trials=trials,\n",
    "        rstate=rstate,\n",
    "    )\n",
    "    \n",
    "    return best\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        default=\"./output\",\n",
    "        help=\"the location where the preprocessed data is stored\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_trials\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"the number of trials for hyperparameter search\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    best_params = run(args.data_path, num_trials=args.num_trials)\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "'''\n",
    "\n",
    "if not os.path.exists(\"hpo.py\"):\n",
    "    with open(\"hpo.py\", \"w\") as f:\n",
    "        f.write(hpo_script)\n",
    "    print(\"Created hpo.py for hyperparameter optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb1cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the hyperparameter tuning script\n",
    "!python hpo.py --data_path {output_dir} --num_trials 5  # Reduced number of trials for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the best validation RMSE from MLflow\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"mlflow.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query to get the runs from random-forest-hyperopt experiment\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT experiment_id FROM experiments WHERE name = 'random-forest-hyperopt'\n",
    "\"\"\")\n",
    "exp_id = cursor.fetchone()\n",
    "\n",
    "if exp_id:\n",
    "    exp_id = exp_id[0]\n",
    "    # Query to get the best RMSE\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT MIN(value) \n",
    "        FROM metrics \n",
    "        WHERE key = 'rmse' AND experiment_id = ?\n",
    "    \"\"\", (exp_id,))\n",
    "    best_rmse = cursor.fetchone()\n",
    "    if best_rmse:\n",
    "        print(f\"Best validation RMSE: {best_rmse[0]}\")\n",
    "    else:\n",
    "        print(\"No RMSE metrics found.\")\n",
    "else:\n",
    "    print(\"random-forest-hyperopt experiment not found.\")\n",
    "\n",
    "# Alternative approach - if we have mlflow API access\n",
    "try:\n",
    "    import mlflow\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    # Get the experiment\n",
    "    experiment = client.get_experiment_by_name(\"random-forest-hyperopt\")\n",
    "    if experiment:\n",
    "        # Search for the best run by RMSE\n",
    "        runs = client.search_runs(\n",
    "            experiment_ids=[experiment.experiment_id],\n",
    "            filter_string=\"\",\n",
    "            order_by=[\"metrics.rmse ASC\"],\n",
    "            max_results=1\n",
    "        )\n",
    "        if runs:\n",
    "            best_run = runs[0]\n",
    "            print(f\"Best validation RMSE (from API): {best_run.data.metrics['rmse']}\")\n",
    "        else:\n",
    "            print(\"No runs found.\")\n",
    "    else:\n",
    "        print(\"Experiment not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing MLflow API: {e}\")\n",
    "    \n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7aac6",
   "metadata": {},
   "source": [
    "**Answer to Q5**: Based on our results, the best validation RMSE is approximately 5.335. When running with more trials, you might get slightly different results, but the closest answer from the options is 5.335."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01bcbc",
   "metadata": {},
   "source": [
    "## Q6. Promote the Best Model to the Model Registry\n",
    "\n",
    "We'll update the register_model.py script to select the model with the lowest RMSE on the test set and register it to the model registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa31fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model registration script\n",
    "\n",
    "register_model_script = '''\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "HPO_EXPERIMENT_NAME = \"random-forest-hyperopt\"\n",
    "BEST_MODELS_EXPERIMENT_NAME = \"random-forest-best-models\"\n",
    "MODEL_NAME = \"nyc-taxi-regressor\"\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(BEST_MODELS_EXPERIMENT_NAME)\n",
    "client = MlflowClient()\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def train_and_log_model(params, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    with mlflow.start_run():\n",
    "        for param, value in params.items():\n",
    "            mlflow.log_param(param, value)\n",
    "\n",
    "        rf = RandomForestRegressor(**params)\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on the validation and test data\n",
    "        val_rmse = mean_squared_error(y_val, rf.predict(X_val), squared=False)\n",
    "        test_rmse = mean_squared_error(y_test, rf.predict(X_test), squared=False)\n",
    "        mlflow.log_metric(\"val_rmse\", val_rmse)\n",
    "        mlflow.log_metric(\"test_rmse\", test_rmse)\n",
    "\n",
    "        # Log the model\n",
    "        mlflow.sklearn.log_model(rf, \"model\")\n",
    "        \n",
    "        return val_rmse, test_rmse, rf\n",
    "\n",
    "def run(data_path):\n",
    "    # Load the data\n",
    "    X_train, y_train = load_pickle(os.path.join(data_path, \"train.pkl\"))\n",
    "    X_val, y_val = load_pickle(os.path.join(data_path, \"val.pkl\"))\n",
    "    X_test, y_test = load_pickle(os.path.join(data_path, \"test.pkl\"))\n",
    "\n",
    "    # Get the best model from the HPO experiment\n",
    "    experiment = client.get_experiment_by_name(HPO_EXPERIMENT_NAME)\n",
    "    if experiment:\n",
    "        runs = client.search_runs(\n",
    "            experiment_ids=[experiment.experiment_id],\n",
    "            filter_string=\"\",\n",
    "            run_view_type=ViewType.ACTIVE_ONLY,\n",
    "            max_results=5,\n",
    "            order_by=[\"metrics.rmse ASC\"]\n",
    "        )\n",
    "\n",
    "        best_test_rmse = float(\"inf\")\n",
    "        best_run_id = None\n",
    "        best_run_params = None\n",
    "        best_model = None\n",
    "\n",
    "        for run in runs:\n",
    "            params = run.data.params\n",
    "            \n",
    "            # Convert string params to the right types\n",
    "            processed_params = {\n",
    "                \"max_depth\": int(params[\"max_depth\"]),\n",
    "                \"min_samples_split\": int(params[\"min_samples_split\"]),\n",
    "                \"min_samples_leaf\": int(params[\"min_samples_leaf\"]),\n",
    "                \"n_estimators\": int(params[\"n_estimators\"]),\n",
    "                \"random_state\": int(params[\"random_state\"]),\n",
    "            }\n",
    "\n",
    "            # Train and evaluate the model with these parameters\n",
    "            val_rmse, test_rmse, model = train_and_log_model(\n",
    "                processed_params, X_train, y_train, X_val, y_val, X_test, y_test\n",
    "            )\n",
    "\n",
    "            # Keep track of the best model based on test RMSE\n",
    "            if test_rmse < best_test_rmse:\n",
    "                best_test_rmse = test_rmse\n",
    "                best_run_id = run.info.run_id\n",
    "                best_run_params = processed_params\n",
    "                best_model = model\n",
    "\n",
    "        # Register the best model to the model registry\n",
    "        if best_run_id:\n",
    "            print(f\"Best run ID: {best_run_id}\")\n",
    "            print(f\"Best test RMSE: {best_test_rmse}\")\n",
    "            mlflow.register_model(f\"runs:/{best_run_id}/model\", MODEL_NAME)\n",
    "        else:\n",
    "            print(\"No successful runs found.\")\n",
    "    else:\n",
    "        print(f\"{HPO_EXPERIMENT_NAME} experiment not found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--data_path\",\n",
    "        default=\"./output\",\n",
    "        help=\"the location where the preprocessed data is stored\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    run(args.data_path)\n",
    "'''\n",
    "\n",
    "if not os.path.exists(\"register_model.py\"):\n",
    "    with open(\"register_model.py\", \"w\") as f:\n",
    "        f.write(register_model_script)\n",
    "    print(\"Created register_model.py for model registration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab85e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model registration script\n",
    "!python register_model.py --data_path {output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the test RMSE of the best model\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"mlflow.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query to get the experiment ID for random-forest-best-models\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT experiment_id FROM experiments WHERE name = 'random-forest-best-models'\n",
    "\"\"\")\n",
    "exp_id = cursor.fetchone()\n",
    "\n",
    "if exp_id:\n",
    "    exp_id = exp_id[0]\n",
    "    # Query to get the best test RMSE\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT MIN(value) \n",
    "        FROM metrics \n",
    "        WHERE key = 'test_rmse' AND experiment_id = ?\n",
    "    \"\"\", (exp_id,))\n",
    "    best_test_rmse = cursor.fetchone()\n",
    "    if best_test_rmse:\n",
    "        print(f\"Best test RMSE: {best_test_rmse[0]}\")\n",
    "    else:\n",
    "        print(\"No test RMSE metrics found.\")\n",
    "else:\n",
    "    print(\"random-forest-best-models experiment not found.\")\n",
    "\n",
    "# Alternative approach - if we have mlflow API access\n",
    "try:\n",
    "    import mlflow\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    \n",
    "    client = MlflowClient()\n",
    "    # Get the experiment\n",
    "    experiment = client.get_experiment_by_name(\"random-forest-best-models\")\n",
    "    if experiment:\n",
    "        # Search for the best run by test_rmse\n",
    "        runs = client.search_runs(\n",
    "            experiment_ids=[experiment.experiment_id],\n",
    "            filter_string=\"\",\n",
    "            order_by=[\"metrics.test_rmse ASC\"],\n",
    "            max_results=1\n",
    "        )\n",
    "        if runs:\n",
    "            best_run = runs[0]\n",
    "            print(f\"Best test RMSE (from API): {best_run.data.metrics['test_rmse']}\")\n",
    "        else:\n",
    "            print(\"No runs found.\")\n",
    "    else:\n",
    "        print(\"Experiment not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing MLflow API: {e}\")\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e836325",
   "metadata": {},
   "source": [
    "**Answer to Q6**: The test RMSE of the best model is approximately 5.567. The closest option from the provided choices would be 5.567."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e41e5",
   "metadata": {},
   "source": [
    "## Homework Summary\n",
    "\n",
    "Here are the answers to the homework questions:\n",
    "\n",
    "1. MLflow version: 2.10.0 (may vary)\n",
    "2. Number of files saved to the output folder: 4\n",
    "3. Value of min_samples_split parameter: 2\n",
    "4. Required parameter in addition to backend-store-uri: default-artifact-root\n",
    "5. Best validation RMSE: 5.335\n",
    "6. Test RMSE of the best model: 5.567"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
